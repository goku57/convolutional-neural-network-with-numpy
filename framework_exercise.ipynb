{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Fundamentals - Neural Networks - Exercise: Neural Network Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Your main goal is to extend the existing framework, to perform experiments with different model combinations and to document your observations. Here is a list of necessary tasks and some ideas for additional points:\n",
    "\n",
    "  * (5) 1 to 5 points are given for improving the class and method comments in the framework files. Points are given based on the quality and quantity of the comments.\n",
    "  * (2) Implement `Dropout` in `layer.py` and test your implementation with a toy example. Create and train a model that includes Dropout as a layer.\n",
    "  * (5) Implement `Batchnorm` in `layer.py` and test your implementation with a toy example. Create and train a model that includes Dropout as a layer.\n",
    "  * (5) Do something extra, up to 5 points.  \n",
    "  \n",
    "Please document thoroughly and explain what you do in your experiments, so that work in the notebook is comprehensible, else no points are given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom\n",
    "from htw_nn_framework.networks import NeuralNetwork\n",
    "from htw_nn_framework.layer import *\n",
    "from htw_nn_framework.activation_func import *\n",
    "from htw_nn_framework.loss_func import *\n",
    "from htw_nn_framework.optimizer import *\n",
    "\n",
    "# third party\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "mnist data directory already exists, download aborted\n",
      "(60000, 28, 28) (60000,)\n",
      "(60000, 1, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# create mnist loader from deep_teaching_commons\n",
    "mnist_loader = Mnist(data_dir='data')\n",
    "\n",
    "# load all data, labels are one-hot-encoded, images are flatten and pixel squashed between [0,1]\n",
    "train_images, train_labels, test_images, test_labels = mnist_loader.get_all_data(flatten=False, one_hot_enc=False, normalized=True)\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# reshape to match generell framework architecture \n",
    "train_images, test_images = train_images.reshape(60000, 1, 28, 28), test_images.reshape(10000, 1, 28, 28)            \n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "train_images, train_labels = train_images[shuffle_index], train_labels[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Fully Connected Network Example\n",
    "This model and optimization is taken from `framework_exercise.ipynb` as an example for a typical pipeline using the framework files."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(784, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn = NeuralNetwork(fcn_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn = Optimizer.sgd(fcn, train_images, train_labels, LossCriteria.cross_entropy_softmax, batch_size=64, epoch=5, learning_rate=0.01, X_test=test_images, y_test=test_labels, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Extensions and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1288, 1850)\n",
      "(1288,)\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# for our network we use the data directly\n",
    "X = lfw_people.data\n",
    "print(X.shape)\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "print(y.shape)\n",
    "print(target_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting, normalizing and randomizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (966, 1850) ,  (966,)\n",
      "test shapes: (322, 1850) ,  (322,)\n",
      "3\n",
      "max values 1.0\n",
      "min value 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(966)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "print(\"train shapes:\", X_train.shape, \", \", y_train.shape)\n",
    "print(\"test shapes:\", X_test.shape, \", \", y_test.shape)\n",
    "print(y_train[0])\n",
    "\n",
    "# to get our matrix values inbetween 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(\"max values\", np.max(X_train))\n",
    "print(\"min value\", np.max(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some cropping magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (966, 1849) ,  (966,)\n",
      "test shapes: (322, 1849) ,  (322,)\n"
     ]
    }
   ],
   "source": [
    "# cropping magic in order to get squared images\n",
    "X_train = X_train[:, 0:1849]\n",
    "X_test = X_test[:, 0:1849]\n",
    "\n",
    "print(\"train shapes:\", X_train.shape, \", \", y_train.shape)\n",
    "print(\"test shapes:\", X_test.shape, \", \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels to binary vector - not needed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)\n",
    "print(\"train shapes:\",X_train.shape,\", \", y_train.shape)\n",
    "print(\"test shapes:\",X_test.shape,\", \", y_test.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some reshape magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(966, 1, 43, 43) (322, 1, 43, 43)\n"
     ]
    }
   ],
   "source": [
    "# reshape to match generell framework architecture\n",
    "X_train, X_test = X_train.reshape(\n",
    "    966, 1, 43, 43), X_test.reshape(322, 1, 43, 43)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing same MNIST Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_lfw_test_relu():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(1849, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 7)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "print(\"ReLU test\")\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_relu = NeuralNetwork(fcn_lfw_test_relu(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn_relu = Optimizer.sgd(fcn_relu, X_train, y_train, LossCriteria.cross_entropy_softmax,\n",
    "                    batch_size=64, epoch=5, learning_rate=0.0001, X_test=X_test,\n",
    "                    y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing activation functions and optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Leaky Relu and sgd_momentum + nesterov"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_lfw():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(1849, 500)\n",
    "    relu_01 = LeakyReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = LeakyReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = LeakyReLU()\n",
    "    ouput = FullyConnected(100, 7)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "print(\"Leaky Relu + sgd momentum test\")\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_test_momentum = NeuralNetwork(fcn_lfw(), score_func=LossCriteria.softmax)\n",
    "# optimize the network and a softmax loss\n",
    "fcn_test_momentum = Optimizer.sgd_momentum(fcn_test_momentum, X_train, y_train, LossCriteria.cross_entropy_softmax,\n",
    "                    batch_size=64, epoch=5, learning_rate=0.00001,mu=0.9, X_test=X_test,\n",
    "                    y_test=y_test, verbose=True)\n",
    "print(\"Leaky Relu + sgd nesterov test\")\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_test_nest = NeuralNetwork(fcn_lfw(), score_func=LossCriteria.softmax)\n",
    "fcn_test_nest = Optimizer.sgd_momentum(fcn_test_nest, X_train, y_train, LossCriteria.cross_entropy_softmax,\n",
    "                    batch_size=64, epoch=5, learning_rate=0.00001,mu=0.9,nesterov = 1, X_test=X_test,\n",
    "                    y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Sigmoid and adagrad + rmsprop"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_lfw():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(1849, 500)\n",
    "    relu_01 = sigmoid()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = sigmoid()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = sigmoid()\n",
    "    ouput = FullyConnected(100, 7)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "print(\"sigmoid + adagrad\")\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_test_adagrad = NeuralNetwork(fcn_lfw(), score_func=LossCriteria.softmax)\n",
    "# optimize the network and a softmax loss\n",
    "fcn_test_adagrad = Optimizer.adagrad(fcn_test_adagrad, X_train, y_train, LossCriteria.cross_entropy_softmax,\n",
    "                                     batch_size=64, epoch=5, learning_rate=0.00001, X_test=X_test,\n",
    "                                     y_test=y_test, verbose=True)\n",
    "print(\"sigmoid + rmsprop\")\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_test_rms = NeuralNetwork(fcn_lfw(), score_func=LossCriteria.softmax)\n",
    "fcn_test_rms = Optimizer.rmsprop(fcn_test_rms, X_train, y_train, LossCriteria.cross_entropy_softmax,\n",
    "                                 batch_size=64, epoch=5, learning_rate=0.0005, X_test=X_test,\n",
    "                                 y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing tan hyperbolic and adadelta + adam"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_lfw():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(1849, 500)\n",
    "    relu_01 = tanh()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = tanh()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = tanh()\n",
    "    ouput = FullyConnected(100, 7)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "\n",
    "print(\"tanh + adadelta\")\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_test_adadelta = NeuralNetwork(fcn_lfw(), score_func=LossCriteria.softmax)\n",
    "# optimize the network and a softmax loss\n",
    "fcn_test_adadelta = Optimizer.adadelta(fcn_test_adadelta, X_train, y_train, LossCriteria.cross_entropy_softmax,\n",
    "                    batch_size=64, epoch=10,mu=0.1,learning_rate=1, X_test=X_test,\n",
    "                    y_test=y_test, verbose=True)\n",
    "\n",
    "print(\"tanh + adam\")\n",
    "#create a neural network on specified architecture with softmax as score function\n",
    "fcn_test_adam = NeuralNetwork(fcn_lfw(), score_func=LossCriteria.softmax)\n",
    "fcn_test_adam = Optimizer.adam(fcn_test_adam, X_train, y_train, LossCriteria.cross_entropy_softmax,\n",
    "                    batch_size=64, epoch=0,learning_rate=0.01, X_test=X_test,\n",
    "                    y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv + Pooling Layer Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenmi/code/htw/uebung 3/htw_nn_framework/loss_func.py:22: RuntimeWarning: divide by zero encountered in log\n",
      "  log_likelihood = -np.log(p[range(m), y])\n"
     ]
    }
   ],
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "\n",
    "\n",
    "def fcn_conv_test():\n",
    "    conv_01 = Conv(1, 32, (3, 3), stride=2, padding=0)\n",
    "    conv_02 = Conv(32, 64, (3, 3), stride=2, padding=0)\n",
    "    max_pooling = Pool()\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(576, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [conv_01, conv_02, max_pooling, flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_conv_test_1 = NeuralNetwork(\n",
    "    fcn_conv_test(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn_conv_test_1 = Optimizer.adam(fcn_conv_test_1, train_images, train_labels, LossCriteria.cross_entropy_softmax,\n",
    "                                     batch_size=128, epoch=10, learning_rate=0.0001, X_test=test_images, y_test=test_labels, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LFW test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 4.752318926052453 :: Training = 0.2505175983436853 :: Test = 0.2453416149068323\n",
      "Epoch 2\n",
      "Loss = 1.7507978155072184 :: Training = 0.3612836438923395 :: Test = 0.3695652173913043\n",
      "Epoch 3\n",
      "Loss = 1.6860057606400363 :: Training = 0.3944099378881988 :: Test = 0.422360248447205\n",
      "Epoch 4\n",
      "Loss = 1.6072208893237012 :: Training = 0.39751552795031053 :: Test = 0.453416149068323\n",
      "Epoch 5\n",
      "Loss = 1.5867796036746022 :: Training = 0.39751552795031053 :: Test = 0.4472049689440994\n",
      "Epoch 6\n",
      "Loss = 1.6253500838414503 :: Training = 0.39648033126293997 :: Test = 0.43788819875776397\n",
      "Epoch 7\n",
      "Loss = 1.5651489034428692 :: Training = 0.39544513457556935 :: Test = 0.4440993788819876\n",
      "Epoch 8\n",
      "Loss = 1.5399461818259794 :: Training = 0.39544513457556935 :: Test = 0.4440993788819876\n",
      "Epoch 9\n",
      "Loss = 1.5266031430415952 :: Training = 0.386128364389234 :: Test = 0.40062111801242234\n",
      "Epoch 10\n",
      "Loss = 1.5015057386908723 :: Training = 0.38819875776397517 :: Test = 0.3944099378881988\n",
      "Epoch 11\n",
      "Loss = 1.46845107659631 :: Training = 0.38509316770186336 :: Test = 0.40683229813664595\n",
      "Epoch 12\n",
      "Loss = 1.4135312325935483 :: Training = 0.3778467908902691 :: Test = 0.38198757763975155\n",
      "Epoch 13\n",
      "Loss = 1.3557185818682533 :: Training = 0.37991718426501037 :: Test = 0.38198757763975155\n",
      "Epoch 14\n",
      "Loss = 1.2954224437329043 :: Training = 0.3612836438923395 :: Test = 0.37888198757763975\n",
      "Epoch 15\n",
      "Loss = 1.241079785328053 :: Training = 0.37370600414078675 :: Test = 0.36645962732919257\n",
      "Epoch 16\n",
      "Loss = 1.2388683665671312 :: Training = 0.37681159420289856 :: Test = 0.36335403726708076\n",
      "Epoch 17\n",
      "Loss = 1.1910344548102767 :: Training = 0.3695652173913043 :: Test = 0.36335403726708076\n",
      "Epoch 18\n",
      "Loss = 1.3559577391959525 :: Training = 0.33954451345755693 :: Test = 0.3385093167701863\n",
      "Epoch 19\n",
      "Loss = 1.1324337553627453 :: Training = 0.35610766045548653 :: Test = 0.36024844720496896\n",
      "Epoch 20\n",
      "Loss = 1.0282124739306513 :: Training = 0.34265010351966874 :: Test = 0.32298136645962733\n",
      "Epoch 21\n",
      "Loss = 0.9712607999398009 :: Training = 0.3281573498964803 :: Test = 0.32298136645962733\n",
      "Epoch 22\n",
      "Loss = 0.8886782413444855 :: Training = 0.3250517598343685 :: Test = 0.32608695652173914\n",
      "Epoch 23\n",
      "Loss = 0.8304831963215547 :: Training = 0.31780538302277433 :: Test = 0.30434782608695654\n",
      "Epoch 24\n",
      "Loss = 0.7976790698383889 :: Training = 0.3053830227743271 :: Test = 0.30745341614906835\n",
      "Epoch 25\n",
      "Loss = 0.7560407415571369 :: Training = 0.30745341614906835 :: Test = 0.30745341614906835\n",
      "Epoch 26\n",
      "Loss = 0.6876380577330728 :: Training = 0.30434782608695654 :: Test = 0.2981366459627329\n",
      "Epoch 27\n",
      "Loss = 0.7500117549056361 :: Training = 0.3053830227743271 :: Test = 0.3167701863354037\n",
      "Epoch 28\n",
      "Loss = 0.6962649303975292 :: Training = 0.3022774327122153 :: Test = 0.30124223602484473\n",
      "Epoch 29\n",
      "Loss = 0.5724259140243886 :: Training = 0.2981366459627329 :: Test = 0.2857142857142857\n",
      "Epoch 30\n",
      "Loss = 0.5436891911367681 :: Training = 0.29296066252587993 :: Test = 0.2795031055900621\n",
      "Epoch 31\n",
      "Loss = 0.4564523728147519 :: Training = 0.2888198757763975 :: Test = 0.27639751552795033\n",
      "Epoch 32\n",
      "Loss = 0.504486440311885 :: Training = 0.2908902691511387 :: Test = 0.30434782608695654\n",
      "Epoch 33\n",
      "Loss = 0.3937568131568535 :: Training = 0.2950310559006211 :: Test = 0.2981366459627329\n",
      "Epoch 34\n",
      "Loss = 0.3364435573701726 :: Training = 0.2908902691511387 :: Test = 0.2826086956521739\n",
      "Epoch 35\n",
      "Loss = 0.431571060721839 :: Training = 0.2950310559006211 :: Test = 0.27639751552795033\n",
      "Epoch 36\n",
      "Loss = 0.3712393915247091 :: Training = 0.29296066252587993 :: Test = 0.2950310559006211\n",
      "Epoch 37\n",
      "Loss = 1.1147624041466688 :: Training = 0.2898550724637681 :: Test = 0.2732919254658385\n",
      "Epoch 38\n",
      "Loss = 0.7816955322170018 :: Training = 0.3281573498964803 :: Test = 0.30434782608695654\n",
      "Epoch 39\n",
      "Loss = 0.5242013310295776 :: Training = 0.32401656314699795 :: Test = 0.2795031055900621\n",
      "Epoch 40\n",
      "Loss = 0.38762352018421625 :: Training = 0.32608695652173914 :: Test = 0.2919254658385093\n",
      "Epoch 41\n",
      "Loss = 0.31609263418099887 :: Training = 0.31262939958592134 :: Test = 0.2795031055900621\n",
      "Epoch 42\n",
      "Loss = 0.3351805629173304 :: Training = 0.2898550724637681 :: Test = 0.2639751552795031\n",
      "Epoch 43\n",
      "Loss = 0.3223155409126754 :: Training = 0.2888198757763975 :: Test = 0.2608695652173913\n",
      "Epoch 44\n",
      "Loss = 0.19914974941927197 :: Training = 0.2826086956521739 :: Test = 0.2608695652173913\n",
      "Epoch 45\n",
      "Loss = 0.28277273210351644 :: Training = 0.31262939958592134 :: Test = 0.2732919254658385\n",
      "Epoch 46\n",
      "Loss = 0.21795583886396958 :: Training = 0.3198757763975155 :: Test = 0.2639751552795031\n",
      "Epoch 47\n",
      "Loss = 0.14452781496396025 :: Training = 0.2981366459627329 :: Test = 0.2577639751552795\n",
      "Epoch 48\n",
      "Loss = 0.21121122856887076 :: Training = 0.2908902691511387 :: Test = 0.2826086956521739\n",
      "Epoch 49\n",
      "Loss = 0.12328994767127546 :: Training = 0.2815734989648033 :: Test = 0.2577639751552795\n",
      "Epoch 50\n",
      "Loss = 0.13097582964078142 :: Training = 0.2981366459627329 :: Test = 0.2701863354037267\n"
     ]
    }
   ],
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_conv_test():\n",
    "    conv_01 = Conv(1,8, (3, 3), stride=2, padding=0)\n",
    "    conv_02 = Conv(8,16, (3, 3), stride=2, padding=0)\n",
    "    max_pooling = Pool()\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(400, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 7)\n",
    "    return [conv_01,conv_02,max_pooling,flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn_conv_test_2 = NeuralNetwork(fcn_conv_test(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn_conv_test_2 = Optimizer.adam(fcn_conv_test_2, X_train, y_train, LossCriteria.cross_entropy_softmax, batch_size=64,\n",
    "                    epoch=50, learning_rate=0.01, X_test=X_test, y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
